System Design Summary: Real-Time QoS Monitoring at Scale
1. Ingestion & Scalability
Source Optimization: Millions of video players (producers) se data lene ke liye humne Client-side Batching ka use kiya (e.g., sending events every 10s). Isse backend load 10x kam ho gaya.

Kafka Layer 1 (Raw Events): Data ko user_id par partition kiya gaya taaki load barabar distribute ho aur "Hot Partitions" (jaise bade ISPs) ki wajah se system crash na ho.

2. Multi-Stage Aggregation (The "Two-Layer" Kafka Approach)
Aggregating 1M EPS directly expensive hota hai, isliye humne ise divide kiya:

Intermediate Workers (Stage 1): Ye workers Raw Kafka se data read karte hain aur memory mein "Local Aggregates" (ISP + Region level counters) banate hain.

Stage 1 workers ka sample output aggregated over 1 minute for a isp and region for a metric

{
  "isp": "Airtel",
  "region": "Mumbai",
  "metric": "critical_errors",
  "count": 450,
  "window_ts": "2023-10-27T10:00:00Z"
}

Kafka Layer 2 (Aggregated Topic): Stage 1 workers apna aggregated data is dusri pipeline mein bhejte hain, jo isp_name par partitioned hai.

Final Consumer: Kyunki data ab already summarized aur ISP-grouped hai, final worker ise asani se ClickHouse mein push kar deta hai for real-time dashboards.

Stage 2 consumers only clickhouse mein data persist karta hai, stage 2 consumer messages ko memory mein 5s k liye buffer karta hai
phir clickhouse mein bulkwrite karta hai

Data Row in ClickHouse:
| timestamp | isp_name | region | total_sessions | error_count | p99_latency_ms | throughput_gbps |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 2023-10-27 10:02:00 | Airtel | Maharashtra | 150000 | 450 | 1200 | 45.5 |
| 2023-10-27 10:02:00 | Jio | Karnataka | 210000 | 110 | 850 | 62.1 |

Stage 2 consumer ne already aggregated aur ISP-partitioned data ko ClickHouse mein "Near Real-Time" (har 1-2 min) persist kar diya hai,
isliye aapka dashboard ya manual query instantly result degi.

3. Investigation & Storage Strategy
Humne "Happy Users" ka raw data store na karke cost bachayi:

Filtering: Stage 1 workers sirf "Critical Errors" (buffering, quality drop) ko filter karte hain.

S3 Data Lake: Ye filtered errors S3 mein store hote hain. Humne Folder-based Partitioning use ki: /isp/region/dt/hr/min/.

Querying: Engineer dashboard par issue dekh kar AWS Athena ke through specific S3 folders ko scan karta hai taaki affected users ki list mil sake.

4. High Availability & Fault Tolerance (The "Resilience" Layer)
Agar main systems down honge, toh data loss rokne ke liye humne ye design kiya:

Scenario A: ClickHouse / S3 is Down
Level 1 (Immediate Retry): Consumer 3 baar retry karega.

Level 2 (Side Kafka Topic): Agar fir bhi fail hota hai, toh data ek Retry-Kafka-Topic mein jayega.

Level 3 (Dead Letter Table): Agar Kafka retry limit bhi exhaust ho jaye, toh data ek Persistent DB Table mein as a "Dead Letter" save hoga.

Scenario B: S3 Regional Failure
Secondary Bucket: Humne ek fallback mechanism rakha jahan primary region (e.g., US-East) down hone par data secondary region (e.g., Mumbai) ki S3 bucket mein switch ho jayega.

Relay Workers: Jab system wapas online aayega, ye workers Dead Letter Table ya Side-Topics se data utha kar main stores mein "Drain" karenge.

Key Architectural Principles Applied:
Decoupling: Processing aur Storage ko alag rakha taaki ek ki failure dusre ko impact na kare.

Efficiency: Aggregation ko distribute kiya taaki koi single node bottleneck na bane.

Durability: Har step par backup storage (Kafka/DB) ka provision rakha.

Ye aapka complete blueprint hai. Aaj ki discussion ne dikhaya ki kaise simple "counters" ko ek massive scale monitoring tool mein convert kiya ja sakta hai.

This is a very good separation of concern example because we have different workers who are computing and which are the aggregating so which are CPU heavy compute heavy and we have another set of consumers which are only IO heavy
which are only writing to the click house. So we can independently scale both the workers and both of them are decoupled from each other, one is CPU have you one is IO heavy and let's say the click house is down. The only the stage two workers will get affected that will not be any impact in the stage one workers
and they will keep on computing and pushing the aggregated data into the stage 2 kafka, we can scale the compute by adding more workers.

Notes for separation of concerns :

1. Independent Scalability (Resource Optimization):

Stage 1 (CPU-Heavy): These workers perform windowing, group-by, and mathematical calculations. If your traffic spikes (like during Flipkart's Big Billion Day ), you can scale these horizontally without wasting money on database connections.

Stage 2 (I/O-Heavy): These workers focus on network throughput and buffering for ClickHouse. You only need a few high-bandwidth instances here, even if your Stage 1 fleet is hundreds of nodes large.

2. Fault Isolation & Backpressure:

Scenario (ClickHouse is down): As you noted, Stage 1 keeps running. It continues to aggregate and push to the Layer 2 Kafka topic.

The Buffer: Kafka acts as a massive buffer. When ClickHouse comes back online, Stage 2 workers can "drain" the backlog at maximum speed. Your 99.99%+ uptime  goal is preserved because the "Compute" layer never lost a single event.


Additional notes :

There is 1 more consumer, Consumer B (The Investigator), it completes the architecture by providing the "Why" and the "Who" after the ClickHouse dashboard identifies the "What."

Here is how that specific flow works:

1. The Filtering Logic (S3 Consumer)
As you noted, this consumer is highly selective. It ignores millions of "Happy Path" events and only acts on Error Events.

The Aggregation: It groups affected User IDs by ISP and Region over a small time window (e.g., 5 minutes).

Storage Format: Instead of millions of tiny files, it flushes a single, optimized file (like Parquet or JSON) to S3 every few minutes.

2. The S3 Partitioning Strategy
By using the folder structure you described—s3://monitoring-logs/isp=Airtel/region=Mumbai/year=2024/month=11/day=27/—you are implementing Hive-style partitioning.

Efficiency: When an engineer sees a spike in Airtel-Mumbai on ClickHouse, they don't have to scan the entire S3 bucket. They point their query (via AWS Athena) directly to that specific folder.

Cost: Scanning only the "Airtel" folder costs a fraction of a full-bucket scan, which is critical for FinOps.

3. The Investigation Workflow
This creates a seamless "Drill-Down" experience:

Detection: You query ClickHouse and see that Airtel's error rate is 15% in Mumbai.

Isolation: You immediately know the Folder Path in S3 because it matches the ISP and Region from the dashboard.

Resolution: You run a query like SELECT user_id FROM s3_table WHERE isp='Airtel' AND region='Mumbai' to get the list of affected users for the support team or to identify a specific IP range that is failing.